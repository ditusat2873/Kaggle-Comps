{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install datasets torch scikit-learn","metadata":{"trusted":true,"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing       import OneHotEncoder\nfrom sklearn.compose             import ColumnTransformer\nfrom sklearn.pipeline            import Pipeline\nfrom xgboost                     import XGBClassifier\nfrom sklearn.metrics             import classification_report\n\ntrain = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntrain.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Need to fill in all the NaNs with 'unkown' in both train and test sets\nfor df in (train, test):\n    df['keyword']  = df['keyword'].fillna('unknown')\n    df['location'] = df['location'].fillna('unknown')\ntest.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#time to simply remove bloating characters.\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\.\\S+', '', text) #remove urls\n    text = re.sub(r'@\\w+', '', text) #remove mentions\n    text = re.sub(r'#', '', text) #remove hashtags\n    text = re.sub(r'[^\\w\\s]', ' ', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ntrain[\"Clean_text\"] = train[\"text\"].apply(preprocess_text)\ntest[\"Clean_text\"] = test[\"text\"].apply(preprocess_text)\ntrain.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#OK to remove the old text column.\nfor df in (train, test):\n    df.drop(columns=['text'], errors='ignore', inplace=True)\n\ntest.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Now lets try to get a 80/20 Train/CV split\nfrom sklearn.model_selection import train_test_split\nX = train[['Clean_text','keyword','location']]\ny = train['target']\n\nX_tr, X_val, y_tr, y_val = train_test_split(\n    X, y,\n    stratify=y,\n    test_size=0.2,\n    random_state=42\n)\nprint(\"Train size:\", X_tr.shape, y_tr.shape)\nprint(\"Val   size:\", X_val.shape, y_val.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#what we need to do is go from text -> TFID vectorization for the XGBoost to use. \npreprocessor = ColumnTransformer([\n    ('tfidf', TfidfVectorizer(\n        max_features=20_000,\n        ngram_range=(1,2),\n        min_df=5\n    ), 'Clean_text'),\n    ('cat', OneHotEncoder(handle_unknown='ignore'), ['keyword','location']), #basically doing OHE for us\n])\n\npipe = Pipeline([\n    ('pre', preprocessor),\n    ('xgb', XGBClassifier( #this is the model\n        n_estimators=300,\n        learning_rate=0.1,\n        max_depth=6,\n        use_label_encoder=False,\n        eval_metric='logloss',\n        random_state=42,\n        n_jobs=-1\n    )),\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe.fit(X_tr, y_tr)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = pipe.predict(X_val)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_val, y_pred, digits=4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_full = train[['Clean_text','keyword','location']]\ny_full = train['target']\npipe.fit(X_full, y_full)\n\n# 2) Prepare your test features\nX_test = test[['Clean_text','keyword','location']]\n\n# 3) Predict on test\ntest_preds = pipe.predict(X_test)\n\n# 4) Load the sample submission file\nsubmission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n\n# 5) Overwrite its “target” column with your predictions\nsubmission['target'] = test_preds\n\n# 6) Save to disk\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}